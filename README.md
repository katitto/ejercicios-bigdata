# ğŸš€ Big Data con Python - De Cero a ProducciÃ³n

> **Aprende a procesar millones de registros sin que tu computadora explote**
> Repositorio educativo completo para dominar Big Data con Python, desde conceptos bÃ¡sicos hasta producciÃ³n.

[![GitHub stars](https://img.shields.io/github/stars/TodoEconometria/ejercicios-bigdata?style=social)](https://github.com/TodoEconometria/ejercicios-bigdata/stargazers)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Conectar-blue)](https://www.linkedin.com/in/juangutierrezconsultor/)
[![Web](https://img.shields.io/badge/Web-TodoEconometria-orange)](https://www.todoeconometria.com)

---

## ğŸ¯ Â¿QuÃ© es Esto y Por QuÃ© Existe?

### El Problema

Imagina esto: Tienes un archivo Excel con **5 aÃ±os de ventas** (500,000 filas). Excel se congela. Python con Pandas se queda sin memoria. Tu jefe necesita el anÃ¡lisis **maÃ±ana**.

**Â¿Te suena familiar?**

Este es el problema que enfrentan miles de analistas, cientÃ­ficos de datos y empresas diariamente. Los datos crecen exponencialmente, pero las herramientas tradicionales no escalan.

### La SoluciÃ³n

Este repositorio te enseÃ±a a:

```python
# âŒ Antes: Excel y Pandas bÃ¡sico
df = pd.read_csv("ventas_5_aÃ±os.csv")  # ğŸ’¥ MemoryError
df.groupby("regiÃ³n").sum()              # ğŸŒ 20 minutos

# âœ… DespuÃ©s: Big Data con Python
df = dd.read_csv("ventas_5_aÃ±os.csv")  # âš¡ Carga lazy
df.groupby("regiÃ³n").sum().compute()    # ğŸš€ 2 segundos
```

**Resultado:** Procesas 100GB de datos en tu laptop como si fueran 10MB.

### Por QuÃ© Este Repositorio

Este material surge de **230 horas de curso presencial** donde enseÃ±o Big Data a profesionales. He destilado:

- âœ… **10+ aÃ±os de experiencia** en anÃ¡lisis de datos
- âœ… **Errores comunes** que cometen los principiantes (y cÃ³mo evitarlos)
- âœ… **Mejores prÃ¡cticas** de la industria
- âœ… **Proyectos reales** adaptados para aprender

**No es solo teorÃ­a.** Cada ejercicio estÃ¡ diseÃ±ado para enfrentarte a problemas del mundo real.

---

## ğŸ‘¥ Â¿Para QuiÃ©n es Este Repositorio?

<details>
<summary><b>ğŸ“ Alumnos del Curso Presencial (230 horas)</b></summary>

Si estÃ¡s inscrito en mi curso presencial:

- âœ… Este repo es tu **material de apoyo** completo
- âœ… AquÃ­ encontrarÃ¡s **todos los ejercicios** del curso
- âœ… Puedes practicar **antes, durante y despuÃ©s** de las clases
- âœ… Tienes **soporte directo** en las sesiones presenciales

**Ventaja:** Mientras otros solo tienen diapositivas, tÃº tienes un repositorio completo con cÃ³digo ejecutable.

</details>

<details>
<summary><b>ğŸŒ Autodidactas y Curiosos (Gratis)</b></summary>

Si encontraste este repositorio por tu cuenta:

- âœ… **Todo el contenido es gratuito** y de cÃ³digo abierto
- âœ… Puedes aprender **a tu ritmo** sin presiÃ³n
- âœ… Practica con **ejercicios reales** de Big Data
- âš ï¸ **No incluye soporte** (solo para alumnos presenciales)

**Ventaja:** Material profesional de calidad sin costo, perfecto para tu portafolio.

</details>

<details>
<summary><b>ğŸ’¼ Empresas y Profesionales</b></summary>

Si buscas soluciones para tu empresa:

- âœ… **Portfolio real** de capacidades en Big Data
- âœ… Muestra cÃ³mo **entreno equipos** profesionales
- âœ… **ConsultorÃ­a y capacitaciÃ³n** in-company disponible
- âœ… Proyectos de **anÃ¡lisis de datos a medida**

**Ventaja:** Ve exactamente quÃ© nivel de calidad ofrezco antes de contratarme.

</details>

---

## ğŸ“ Â¿QuÃ© AprenderÃ¡s?

### Roadmap de Aprendizaje

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TU VIAJE EN BIG DATA                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  NIVEL 1: Fundamentos                                       â”‚
â”‚  â”œâ”€ SQLite: Bases de datos relacionales                    â”‚
â”‚  â”œâ”€ Pandas: AnÃ¡lisis de datos en memoria                   â”‚
â”‚  â””â”€ Git/GitHub: Control de versiones                       â”‚
â”‚                                                              â”‚
â”‚           â†“ AprenderÃ¡s en 2-3 semanas                      â”‚
â”‚                                                              â”‚
â”‚  NIVEL 2: Escalando                                         â”‚
â”‚  â”œâ”€ Dask: Procesamiento paralelo (datos > RAM)            â”‚
â”‚  â”œâ”€ Parquet: Almacenamiento columnar eficiente            â”‚
â”‚  â””â”€ OptimizaciÃ³n: TÃ©cnicas de performance                  â”‚
â”‚                                                              â”‚
â”‚           â†“ AprenderÃ¡s en 3-4 semanas                      â”‚
â”‚                                                              â”‚
â”‚  NIVEL 3: Big Data Real                                     â”‚
â”‚  â”œâ”€ PySpark: Procesamiento distribuido                     â”‚
â”‚  â”œâ”€ SQL avanzado: Queries complejas                        â”‚
â”‚  â””â”€ Pipelines: ETL/ELT en producciÃ³n                       â”‚
â”‚                                                              â”‚
â”‚           â†“ AprenderÃ¡s en 4-5 semanas                      â”‚
â”‚                                                              â”‚
â”‚  NIVEL 4: VisualizaciÃ³n y Deploy                            â”‚
â”‚  â”œâ”€ Dashboards: Flask + Chart.js                           â”‚
â”‚  â”œâ”€ APIs: Servir datos procesados                          â”‚
â”‚  â””â”€ Deploy: Poner en producciÃ³n                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â±ï¸ Tiempo total estimado: 10-12 semanas (a tu ritmo)
```

### TecnologÃ­as que DominarÃ¡s

| TecnologÃ­a | QuÃ© Hace | CuÃ¡ndo Usarla |
|------------|----------|---------------|
| **Python** | Lenguaje base | Siempre |
| **Pandas** | Datos en memoria (< 5GB) | AnÃ¡lisis exploratorio |
| **Dask** | Datos > RAM (5-100GB) | Datasets grandes en 1 mÃ¡quina |
| **PySpark** | Datos masivos (> 100GB) | Clusters, producciÃ³n |
| **SQLite** | Base de datos embebida | Prototipos, proyectos pequeÃ±os |
| **Parquet** | Formato columnar | Almacenar datos procesados |
| **Git/GitHub** | Control de versiones | Todo proyecto profesional |
| **Flask** | Web framework | Dashboards, APIs |

### Ejemplos de QuÃ© PodrÃ¡s Hacer

**Ejemplo 1: Analizar 10 Millones de Viajes de Taxi**

```python
# Dataset: NYC Taxi (121 MB CSV, 10M+ registros)
# Pregunta: Â¿CuÃ¡l es el ingreso promedio por hora del dÃ­a?

import dask.dataframe as dd

# Cargar 121 MB como si fueran 10 MB âš¡
df = dd.read_csv("yellow_tripdata_2021-01.csv")

# AnÃ¡lisis que en Pandas tomarÃ­a 5 minutos, aquÃ­: 10 segundos
resultado = (df.groupby(df['tpep_pickup_datetime'].dt.hour)
              ['total_amount']
              .mean()
              .compute())

print(resultado)
# Resultado: Hora 23 es la mÃ¡s rentable ($18.50 promedio)
```

**Ejemplo 2: Dashboard en Tiempo Real**

Crear un dashboard interactivo que muestra:
- ğŸ“Š DistribuciÃ³n de viajes por hora
- ğŸ—ºï¸ Mapa de calor de zonas mÃ¡s rentables
- ğŸ’° Ingresos totales por dÃ­a/semana/mes
- ğŸ“ˆ Tendencias temporales

**Ejemplo 3: Pipeline ETL de ProducciÃ³n**

```
CSV (100GB) â†’ Limpiar â†’ Transformar â†’ Parquet â†’ Dashboard
              (Dask)    (PySpark)    (10GB)     (Flask)
```

---

## ğŸš€ CÃ³mo Empezar (Todos los Niveles)

### NIVEL 0: Primera Vez con Git y Python

<details>
<summary><b>Click aquÃ­ si es tu primera vez</b></summary>

#### Paso 1: Instalar Herramientas BÃ¡sicas

**Windows:**
```bash
# Instalar Git
winget install Git.Git

# Instalar Python
winget install Python.Python.3.11

# Verificar instalaciÃ³n
git --version
python --version
```

**Mac:**
```bash
# Instalar Homebrew (si no lo tienes)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Instalar Git y Python
brew install git python@3.11

# Verificar
git --version
python3 --version
```

**Linux:**
```bash
sudo apt-get update
sudo apt-get install git python3.11 python3-pip
```

#### Paso 2: Configurar Git

```bash
git config --global user.name "Tu Nombre"
git config --global user.email "tu@email.com"
```

#### Paso 3: Crear Cuenta en GitHub

1. Ir a https://github.com/
2. Click "Sign Up"
3. Verificar email

Â¡Listo! Ahora ve a **NIVEL 1** â†“

</details>

---

### NIVEL 1: Tengo Git y GitHub, Â¿Ahora QuÃ©?

<details>
<summary><b>Click aquÃ­ para el flujo completo</b></summary>

## ğŸ“‹ FLUJO COMPLETO DE TRABAJO

### VisiÃ³n General del Flujo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FLUJO DE TRABAJO COMPLETO                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. FORK                                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚  Repositorio del Profesor (Origen)  â”‚                  â”‚
â”‚     â”‚  TodoEconometria/ejercicios-bigdata â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                    â”‚                                          â”‚
â”‚                    â”‚ Hacer Fork                               â”‚
â”‚                    â†“                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚   TU Repositorio (Fork PÃºblico)     â”‚                  â”‚
â”‚     â”‚   TU_USUARIO/ejercicios-bigdata     â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                               â”‚
â”‚  2. CLONE                                                     â”‚
â”‚     git clone https://github.com/TU_USUARIO/...             â”‚
â”‚                    â”‚                                          â”‚
â”‚                    â†“                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚    Repositorio Local (Tu PC)        â”‚                  â”‚
â”‚     â”‚    ejercicios-bigdata/              â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                               â”‚
â”‚  3. CREAR REPO PRIVADO DE PRUEBAS (Opcional pero recomendado)â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚   TU Repo Privado (Experimentos)    â”‚                  â”‚
â”‚     â”‚   TU_USUARIO/bigdata-practica       â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                               â”‚
â”‚  4. TRABAJAR EN EJERCICIOS                                   â”‚
â”‚     - Editar cÃ³digo localmente                               â”‚
â”‚     - Hacer commits frecuentes                               â”‚
â”‚     - Probar en tu repo privado primero                      â”‚
â”‚                                                               â”‚
â”‚  5. PULL REQUEST (Entregar)                                  â”‚
â”‚     TU Fork â†’ Repositorio del Profesor                       â”‚
â”‚                                                               â”‚
â”‚  6. FEEDBACK                                                 â”‚
â”‚     Profesor revisa â†’ Comentarios â†’ Correcciones            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PASO 1: Hacer Fork del Repositorio

**Â¿QuÃ© es un Fork?**

Un fork es TU COPIA PERSONAL del repositorio. PiÃ©nsalo como fotocopiar un libro: el original sigue siendo del autor, pero tÃº puedes escribir en tu copia.

**CÃ³mo hacer Fork:**

1. **Ir al repositorio original:**
   https://github.com/TodoEconometria/ejercicios-bigdata

2. **Click en "Fork"** (botÃ³n arriba a la derecha)

3. **Seleccionar tu cuenta** como destino

4. **Â¡Listo!** Ahora tienes tu propia copia en:
   `https://github.com/TU_USUARIO/ejercicios-bigdata`

**âš ï¸ IMPORTANTE:** Siempre trabaja en TU fork, NO en el repositorio original.

---

### PASO 2: Clonar TU Fork a Tu Computadora

```bash
# âŒ MAL - No clones el original
git clone https://github.com/TodoEconometria/ejercicios-bigdata.git

# âœ… BIEN - Clona TU fork
git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git

# Entrar al directorio
cd ejercicios-bigdata

# Configurar el repositorio original como "upstream"
git remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git

# Verificar remotos
git remote -v
# DeberÃ­as ver:
# origin    â†’ tu fork
# upstream  â†’ repositorio original
```

---

### PASO 3: Crear Tu Repositorio Privado de Pruebas

#### Â¿Por QuÃ© Necesitas un Repo Privado?

**Historia real de lo que nos pasÃ³:**

> *"Estaba desarrollando un ejercicio nuevo. SubÃ­ el borrador al repositorio pÃºblico sin darme cuenta. Los alumnos vieron las soluciones antes de tiempo. Tuve que hacer rollback de emergencia y limpiar el historial de Git. PerdÃ­ 3 horas arreglando el desastre."*
> â€” Experiencia real del profesor

**Lecciones aprendidas:**

1. âŒ **Nunca experimentes directamente en el repositorio pÃºblico**
2. âœ… **Siempre prueba primero en un repo privado**
3. âœ… **Solo publica cuando estÃ©s 100% seguro**

#### OpciÃ³n A: Repositorio Privado Tradicional

**Crear repo privado:**

```bash
# 1. Ir a https://github.com/new

# 2. Configurar:
#    - Repository name: bigdata-practica (o como quieras)
#    - Description: "Mi espacio de prÃ¡ctica para Big Data"
#    - Visibility: ğŸ”’ Private (MUY IMPORTANTE)
#    - Initialize: NO (dejar vacÃ­o)

# 3. Crear repositorio

# 4. En tu computadora, crear carpeta separada:
mkdir ../bigdata-practica
cd ../bigdata-practica

# 5. Inicializar y conectar
git init
git remote add origin https://github.com/TU_USUARIO/bigdata-practica.git

# 6. Copiar ejercicios para experimentar
cp -r ../ejercicios-bigdata/ejercicios/ .
cp -r ../ejercicios-bigdata/datos/ .

# 7. Hacer primer commit
git add .
git commit -m "Setup inicial de prÃ¡ctica"
git push -u origin main
```

**Flujo de trabajo con repo privado:**

```
ejercicios-bigdata/          â† Fork pÃºblico (entregas)
â”‚
â””â”€ ejercicios/
   â””â”€ 01_cargar_sqlite.py

bigdata-practica/            â† Repo privado (experimentos)
â”‚
â”œâ”€ prueba_01.py             â† Experimentas aquÃ­
â”œâ”€ prueba_02_error.py       â† Si falla, no importa
â””â”€ 01_solucion_final.py     â† Cuando funciona, copias al pÃºblico
```

#### OpciÃ³n B: GitHub Codespaces (MÃ¡s FÃ¡cil)

**Â¿QuÃ© es Codespaces?**

Un entorno de desarrollo completo en la nube. Como tener Visual Studio Code en tu navegador.

**Ventajas:**

- âœ… No necesitas instalar nada
- âœ… Funciona desde cualquier computadora
- âœ… Entorno aislado para experimentar
- âœ… 60 horas gratis al mes

**CÃ³mo usar:**

1. En tu fork de GitHub, click en "Code" â†’ "Codespaces" â†’ "Create codespace"

2. Se abre VS Code en el navegador âœ¨

3. Terminal integrada para ejecutar cÃ³digo

4. Experimenta sin miedo - es tu espacio privado

5. Cuando estÃ©s listo, haz commit y push

**RecomendaciÃ³n:** Usa Codespaces para experimentos rÃ¡pidos, repo privado para proyectos serios.

---

### PASO 4: Trabajar en un Ejercicio

#### Workflow Completo

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CADA VEZ QUE EMPIECES UN EJERCICIO NUEVO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Actualizar tu fork con cambios del profesor
git checkout main
git fetch upstream
git merge upstream/main
git push origin main

# 2. Crear una rama para el ejercicio (OPCIONAL pero recomendado)
git checkout -b ejercicio-01

# 3. Ir a la carpeta del ejercicio
cd ejercicios/01_cargar_sqlite.py

# 4. Leer el enunciado COMPLETO
cat ENUNCIADO.md  # Si existe
# O ver comentarios en el archivo .py

# 5. Experimentar primero en tu repo privado (si es complejo)
#    Copiar el archivo a bigdata-practica/
#    Probar diferentes enfoques
#    Cuando funcione, volver al repo pÃºblico

# 6. Trabajar en el ejercicio
code 01_cargar_sqlite.py  # O tu editor favorito

# 7. Ejecutar y probar
python 01_cargar_sqlite.py

# 8. Si funciona, hacer commit
git add 01_cargar_sqlite.py
git commit -m "Ejercicio 01: Implementar carga de datos SQLite"

# 9. Si no funciona, seguir iterando
#    NO hagas commit de cÃ³digo roto

# 10. Cuando estÃ© listo, subir a tu fork
git push origin ejercicio-01  # O main si no creaste rama
```

#### Tips para Resolver Ejercicios

**ğŸ¯ Antes de Empezar:**

```
â–¡ LeÃ­ el enunciado COMPLETO (no solo el tÃ­tulo)
â–¡ EntendÃ­ QUÃ‰ se pide (no cÃ³mo hacerlo todavÃ­a)
â–¡ IdentifiquÃ© los datos de entrada y salida esperada
â–¡ BusquÃ© si hay un archivo AYUDA.md con pistas
```

**ğŸ’» Mientras Trabajo:**

```python
# âœ… BIEN - Desarrolla en pasos pequeÃ±os
# Paso 1: Cargar datos
df = pd.read_csv("datos.csv")
print(df.head())  # Verificar que cargÃ³ bien

# Paso 2: Limpiar datos
df = df.dropna()
print(f"Filas despuÃ©s de limpiar: {len(df)}")

# Paso 3: Analizar
resultado = df.groupby("categoria").sum()
print(resultado)

# âŒ MAL - Escribir todo de golpe
df = pd.read_csv("datos.csv").dropna().groupby("categoria").sum()
# Si falla, no sabes en quÃ© paso fue el error
```

**ğŸ§ª Probar Frecuentemente:**

```bash
# No esperes a terminar todo para probar
python mi_ejercicio.py  # Ejecuta despuÃ©s de cada cambio importante
```

**ğŸ“ Commits Frecuentes:**

```bash
# âœ… BIEN
git commit -m "Ejercicio 01: Agregar funciÃ³n de carga"
git commit -m "Ejercicio 01: Implementar limpieza de nulos"
git commit -m "Ejercicio 01: Agregar anÃ¡lisis estadÃ­stico"

# âŒ MAL
git commit -m "ejercicio terminado"  # Vago, sin contexto
```

---

### PASO 5: Hacer Pull Request (Entregar)

#### Â¿QuÃ© es un Pull Request (PR)?

Un Pull Request es decir: *"Profe, terminÃ© el ejercicio. Â¿Puedes revisarlo?"*

Es como entregar una tarea, pero con superpoderes:
- El profesor ve exactamente QUÃ‰ cambiaste
- Puede comentar lÃ­neas especÃ­ficas de cÃ³digo
- Puedes hacer correcciones despuÃ©s
- Queda registro de todo el proceso

#### CÃ³mo Crear un PR

**1. AsegÃºrate de que tu cÃ³digo funciona:**

```bash
# Ejecuta el ejercicio una Ãºltima vez
python ejercicios/01_cargar_sqlite.py

# Revisa que no hay errores
# Verifica que cumple los requisitos del enunciado
```

**2. Sube tus cambios a tu fork:**

```bash
git push origin ejercicio-01  # O main si trabajaste ahÃ­
```

**3. Ir a GitHub:**

OpciÃ³n A: GitHub te muestra un banner amarillo automÃ¡ticamente:
```
"ejercicio-01 had recent pushes"
[Compare & pull request]  â† Click aquÃ­
```

OpciÃ³n B: Manual:
1. Ir a tu fork: `https://github.com/TU_USUARIO/ejercicios-bigdata`
2. Click en "Pull requests" â†’ "New pull request"
3. Seleccionar:
   - Base repository: `TodoEconometria/ejercicios-bigdata` (base: `main`)
   - Head repository: `TU_USUARIO/ejercicios-bigdata` (compare: `ejercicio-01`)

**4. Completar informaciÃ³n del PR:**

```markdown
TÃ­tulo: Entrega Ejercicio 01: Carga de Datos SQLite - [Tu Nombre]

DescripciÃ³n:
## âœ… Ejercicio Completado
Ejercicio 01: AnÃ¡lisis de datos con SQLite

## ğŸ“ QuÃ© Hice
- ImplementÃ© funciÃ³n para cargar datos desde CSV a SQLite
- AgreguÃ© validaciÃ³n de tipos de datos
- CreÃ© queries SQL para anÃ¡lisis bÃ¡sico
- GenerÃ© reporte de estadÃ­sticas descriptivas

## ğŸ§ª Pruebas Realizadas
- âœ… Probado con dataset de 10,000 registros
- âœ… Probado con datos con valores nulos
- âœ… Verificado que queries devuelven resultados esperados

## â±ï¸ Tiempo Invertido
Aproximadamente 4 horas (incluyendo investigaciÃ³n)

## ğŸ¤” Dificultades Encontradas
- Tuve problemas inicialmente con la codificaciÃ³n UTF-8 del CSV
- Solucionado agregando `encoding='utf-8'` en read_csv()

## ğŸ’¡ Aprendizajes
- AprendÃ­ la diferencia entre SQLite y bases de datos cliente-servidor
- EntendÃ­ cuÃ¡ndo usar Ã­ndices en SQLite
- PractiquÃ© optimizaciÃ³n de queries

## ğŸ“š Recursos Consultados
- DocumentaciÃ³n oficial de SQLite
- Pandas documentation sobre to_sql()
- Stack Overflow para el problema de encoding

## ğŸ™‹ Preguntas para el Profesor
- Â¿Hay una forma mÃ¡s eficiente de hacer bulk insert en SQLite?
- Â¿DeberÃ­a usar transactions para mejorar performance?
```

**5. Click "Create pull request"**

**6. Esperar revisiÃ³n del profesor**

#### DespuÃ©s de Crear el PR

**Si el profesor pide cambios:**

```bash
# 1. Hacer las correcciones en tu cÃ³digo local
# Edita los archivos segÃºn los comentarios

# 2. Commitear los cambios
git add .
git commit -m "Correcciones segÃºn feedback: optimizar queries"

# 3. Subir cambios
git push origin ejercicio-01

# 4. El PR se actualiza automÃ¡ticamente âœ¨
# No necesitas crear un nuevo PR
```

**Si el profesor aprueba:**

ğŸ‰ Â¡Felicitaciones! Tu cÃ³digo fue aceptado.

---

### PASO 6: Mantener tu Fork Actualizado

El profesor subirÃ¡ nuevos ejercicios y actualizaciones. Necesitas sincronizar:

```bash
# Hacer esto SEMANALMENTE o antes de empezar un nuevo ejercicio

# 1. Cambiar a main
git checkout main

# 2. Descargar cambios del profesor
git fetch upstream

# 3. Integrar cambios
git merge upstream/main

# Si hay conflictos (raro), Git te avisarÃ¡
# Resuelve manualmente y haz commit

# 4. Subir actualizaciones a tu fork
git push origin main

# Â¡Listo! Tu fork estÃ¡ actualizado
```

---

</details>

---

### NIVEL 2: Soy Desarrollador, Dame lo Esencial

<details>
<summary><b>TL;DR para devs experimentados</b></summary>

```bash
# Setup (1 minuto)
git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git
cd ejercicios-bigdata
git remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Workflow
git fetch upstream && git merge upstream/main
git checkout -b ejercicio-XX
# ... work ...
git push origin ejercicio-XX
# Create PR on GitHub

# Actualizar
git fetch upstream
git checkout main
git merge upstream/main
git push origin main
```

**Estructura del repo:**
```
ejercicios-bigdata/
â”œâ”€â”€ ejercicios/          # Ejercicios progresivos
â”œâ”€â”€ datos/               # Datasets (NYC Taxi, etc.)
â”œâ”€â”€ dashboards/          # Ejemplos de viz
â””â”€â”€ docs/                # GuÃ­as adicionales
```

**Tech stack:**
- Python 3.11+
- Pandas, Dask, PySpark
- SQLite, Parquet
- Flask para dashboards

</details>

---

## ğŸ“š Ejercicios Disponibles

### Roadmap Detallado

| # | Ejercicio | TecnologÃ­a | Nivel | Tiempo Estimado |
|---|-----------|------------|-------|-----------------|
| 01 | Carga de Datos con SQLite | SQLite + Pandas | ğŸŸ¢ BÃ¡sico | 2-3 horas |
| 02 | Limpieza y TransformaciÃ³n | Pandas | ğŸŸ¢ BÃ¡sico | 3-4 horas |
| 03 | Procesamiento con Parquet y Dask | Dask + Parquet | ğŸŸ¡ Intermedio | 4-5 horas |
| 04 | Queries Complejas con PySpark | PySpark | ğŸŸ¡ Intermedio | 5-6 horas |
| 05 | Dashboard Interactivo | Flask + Chart.js | ğŸ”´ Avanzado | 8-10 horas |
| 06 | Pipeline ETL Completo | Dask + PySpark | ğŸ”´ Avanzado | 10-12 horas |

### Ejercicio 01: Carga de Datos con SQLite

**Â¿QuÃ© aprenderÃ¡s?**
- Cargar datos desde CSV a base de datos
- Queries SQL bÃ¡sicas (SELECT, WHERE, GROUP BY)
- OptimizaciÃ³n con Ã­ndices
- Exportar resultados

**Dataset:** NYC Taxi Trips (121 MB, 10M+ registros)

**DesafÃ­o:** Cargar y analizar datos de taxis sin que tu computadora se congele.

<details>
<summary><b>Ver ejemplo de soluciÃ³n</b></summary>

```python
import sqlite3
import pandas as pd

# Cargar CSV en chunks (por partes)
chunksize = 100000
chunks = pd.read_csv("yellow_tripdata_2021-01.csv", chunksize=chunksize)

# Crear base de datos SQLite
conn = sqlite3.connect("taxi.db")

# Cargar por chunks
for i, chunk in enumerate(chunks):
    chunk.to_sql("trips", conn, if_exists="append", index=False)
    print(f"Chunk {i+1} cargado ({len(chunk)} registros)")

# Crear Ã­ndices para acelerar queries
conn.execute("CREATE INDEX idx_pickup ON trips(tpep_pickup_datetime)")

# Query ejemplo: Promedio de tarifa por hora
query = """
    SELECT
        strftime('%H', tpep_pickup_datetime) as hora,
        AVG(total_amount) as promedio_tarifa,
        COUNT(*) as num_viajes
    FROM trips
    GROUP BY hora
    ORDER BY hora
"""

resultado = pd.read_sql_query(query, conn)
print(resultado)

conn.close()
```

**Output esperado:**
```
hora  promedio_tarifa  num_viajes
00    15.23           234567
01    14.89           198234
02    16.45           165789
...
```

</details>

---

### Ejercicio 02: Limpieza y TransformaciÃ³n

**Â¿QuÃ© aprenderÃ¡s?**
- Detectar y manejar valores nulos
- Identificar outliers
- Transformaciones de datos
- ValidaciÃ³n de tipos

**Dataset:** Mismo NYC Taxi (pero "sucio")

**DesafÃ­o:** Datos del mundo real siempre vienen sucios. Aprender a limpiarlos profesionalmente.

---

### Ejercicio 03: Procesamiento con Parquet y Dask

**Â¿QuÃ© aprenderÃ¡s?**
- Por quÃ© Parquet es mejor que CSV
- Procesamiento paralelo con Dask
- Lazy evaluation
- OptimizaciÃ³n de memoria

**Dataset:** NYC Taxi (convertido a Parquet)

**DesafÃ­o:** Procesar 10GB de datos en una laptop de 8GB RAM.

---

### Ejercicio 04: Queries Complejas con PySpark

**Â¿QuÃ© aprenderÃ¡s?**
- IntroducciÃ³n a Spark
- DataFrames distribuidos
- SQL en Spark
- Particionamiento de datos

**Dataset:** NYC Taxi + Weather Data (join de mÃºltiples fuentes)

**DesafÃ­o:** Combinar datos de diferentes fuentes y hacer anÃ¡lisis complejos.

---

### Ejercicio 05: Dashboard Interactivo

**Â¿QuÃ© aprenderÃ¡s?**
- Flask para backend
- Chart.js para visualizaciones
- Conectar frontend con anÃ¡lisis de datos
- Deploy local

**Proyecto:** Dashboard EDA (Exploratory Data Analysis) de NYC Taxi

**DesafÃ­o:** Crear un dashboard profesional que impresione en entrevistas.

---

## ğŸ”§ Setup del Entorno de Desarrollo

### InstalaciÃ³n de Dependencias

```bash
# Clonar el repositorio (tu fork)
git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git
cd ejercicios-bigdata

# Crear entorno virtual
python -m venv .venv

# Activar entorno virtual
source .venv/bin/activate  # Mac/Linux
.venv\Scripts\activate      # Windows

# Instalar dependencias
pip install -r requirements.txt

# Verificar instalaciÃ³n
python -c "import pandas, dask, pyspark; print('Todo OK!')"
```

### Estructura de Carpetas

```
ejercicios-bigdata/
â”‚
â”œâ”€â”€ .github/                    # Templates de issues y PRs
â”œâ”€â”€ datos/                      # Datasets
â”‚   â”œâ”€â”€ README.md              # CÃ³mo descargar datos
â”‚   â””â”€â”€ descargar_datos.py     # Script automÃ¡tico
â”‚
â”œâ”€â”€ ejercicios/                 # Ejercicios del curso
â”‚   â”œâ”€â”€ 01_cargar_sqlite.py
â”‚   â”œâ”€â”€ 02_limpieza_datos.py
â”‚   â”œâ”€â”€ 03_parquet_dask.py
â”‚   â”œâ”€â”€ 04_pyspark_query.py
â”‚   â””â”€â”€ [futuros ejercicios]
â”‚
â”œâ”€â”€ dashboards/                 # Ejemplos de visualizaciÃ³n
â”‚   â”œâ”€â”€ nyc_taxi_eda/          # Dashboard ejemplo
â”‚   â””â”€â”€ ejemplos-destacados/   # GalerÃ­a de proyectos
â”‚
â”œâ”€â”€ docs/                       # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ CONFIGURACION_INICIAL.md
â”‚   â”œâ”€â”€ GUIA_ENTREGA_DASHBOARDS.md
â”‚   â””â”€â”€ INSTRUCCIONES_ALUMNOS.md
â”‚
â”œâ”€â”€ .gitignore                  # Archivos ignorados por Git
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ PARA_ALUMNOS.md            # Info especÃ­fica del curso
â””â”€â”€ README.md                   # Este archivo
```

---

## ğŸŒŸ Ejemplos Destacados

### Dashboard NYC Taxi EDA

![Dashboard Preview](dashboards/nyc_taxi_eda/preview.png)

**CaracterÃ­sticas:**
- VisualizaciÃ³n interactiva de 10M+ registros
- Filtros dinÃ¡micos por fecha, hora, zona
- Mapas de calor de rutas mÃ¡s rentables
- AnÃ¡lisis de tendencias temporales

**TecnologÃ­as:**
- Backend: Flask + Pandas
- Frontend: Chart.js + Leaflet.js
- Deploy: Docker

**Ver cÃ³digo:** [`dashboards/nyc_taxi_eda/`](dashboards/nyc_taxi_eda/)

---

## â“ Preguntas Frecuentes (FAQ)

<details>
<summary><b>Â¿Necesito experiencia previa en Big Data?</b></summary>

**No.** El curso empieza desde cero. Solo necesitas:
- Conocimientos bÃ¡sicos de Python
- Saber usar la terminal/consola
- Ganas de aprender

Si no tienes experiencia con Python, te recomiendo hacer estos tutoriales primero:
- [Learn Python (Codecademy)](https://www.codecademy.com/learn/learn-python-3)
- [Python for Everybody (Coursera)](https://www.coursera.org/specializations/python)

</details>

<details>
<summary><b>Â¿CuÃ¡nto tiempo toma completar los ejercicios?</b></summary>

**Depende de tu nivel:**

- **Principiantes:** 10-12 semanas (10-15 horas/semana)
- **Intermedios:** 6-8 semanas (8-10 horas/semana)
- **Avanzados:** 4-5 semanas (5-8 horas/semana)

No hay prisa. Aprende a tu ritmo.

</details>

<details>
<summary><b>Â¿Los datos son reales o sintÃ©ticos?</b></summary>

**Reales.** Usamos datasets pÃºblicos reales:
- NYC Taxi & Limousine Commission (TLC)
- Weather data de NOAA
- Otros datasets pÃºblicos de Kaggle

Esto te da experiencia con datos del mundo real (sucios, incompletos, grandes).

</details>

<details>
<summary><b>Â¿Puedo usar esto en mi portafolio?</b></summary>

**Â¡SÃ!** De hecho, te lo recomiendo.

Muchos alumnos han conseguido trabajo mostrando:
- Sus soluciones de los ejercicios
- El dashboard que crearon
- Su fork de GitHub con commits profesionales

Tip: Haz tu fork pÃºblico y agrega un README personalizado explicando tu aprendizaje.

</details>

<details>
<summary><b>Â¿Hay certificado al terminar?</b></summary>

**Para alumnos del curso presencial:** SÃ­, certificado de 230 horas.

**Para autodidactas:** No hay certificado oficial, pero tu GitHub es tu certificado. Los empleadores valoran mÃ¡s ver tu cÃ³digo que un PDF.

</details>

<details>
<summary><b>Â¿QuÃ© computadora necesito?</b></summary>

**MÃ­nimo:**
- 8GB RAM
- 20GB espacio en disco
- Procesador i5 o equivalente

**Recomendado:**
- 16GB RAM
- 50GB espacio en disco SSD
- Procesador i7 o equivalente

**Nota:** Si tienes menos recursos, puedes usar Google Colab o GitHub Codespaces (gratis).

</details>

<details>
<summary><b>Â¿Ofrecen soporte si me atorÃ³?</b></summary>

**Para alumnos del curso presencial:** SÃ­, soporte completo en las sesiones.

**Para autodidactas:** No hay soporte directo, pero puedes:
- Crear un Issue en GitHub con tu pregunta
- Buscar en Issues existentes (probablemente alguien mÃ¡s tuvo tu problema)
- Unirte a la comunidad de Python/Data Science en Discord/Slack

</details>

---

## ğŸ’¼ Servicios Profesionales

### ConsultorÃ­a en Big Data

Â¿Necesitas ayuda con un proyecto de datos en tu empresa?

**Ofrezco:**

- âœ… **Desarrollo de Pipelines ETL/ELT** con Python y Spark
- âœ… **CapacitaciÃ³n Empresarial** (cursos personalizados para tu equipo)
- âœ… **AnÃ¡lisis de Datos** para insights accionables
- âœ… **AutomatizaciÃ³n de Procesos** de datos
- âœ… **MigraciÃ³n a Big Data** (de Excel/SQL a Dask/Spark)

**Casos de uso:**

```
Empresa A: "Tenemos 5 aÃ±os de ventas en Excel y toma 2 horas generar reportes"
â†’ SoluciÃ³n: Pipeline automatizado con Dask + Dashboard en tiempo real
â†’ Resultado: Reportes en 30 segundos

Empresa B: "Queremos capacitar a 15 analistas en Big Data"
â†’ SoluciÃ³n: Curso in-company de 40 horas adaptado a su industria
â†’ Resultado: Equipo autÃ³nomo procesando TB de datos

Startup C: "Necesitamos procesar logs de servidores (1TB/dÃ­a)"
â†’ SoluciÃ³n: Pipeline PySpark en AWS EMR
â†’ Resultado: AnÃ¡lisis en tiempo real con costos optimizados
```

### CapacitaciÃ³n Empresarial

Entreno equipos en:

- **Nivel BÃ¡sico:** Fundamentos de Python para Datos (40 horas)
- **Nivel Intermedio:** Pandas y AnÃ¡lisis Exploratorio (60 horas)
- **Nivel Avanzado:** Big Data con Dask y PySpark (80 horas)
- **Personalizado:** Adaptado a tu industria y tecnologÃ­as

**Modalidades:**
- Presencial (CDMX y Ã¡rea metropolitana)
- Online (Zoom/Teams)
- HÃ­brido

### Contacto

ğŸ“§ **Email:** [cursos@todoeconometria.com](mailto:cursos@todoeconometria.com)
ğŸ’¼ **LinkedIn:** [Juan Gutierrez](https://www.linkedin.com/in/juangutierrezconsultor/)
ğŸŒ **Web:** [www.todoeconometria.com](https://www.todoeconometria.com)

<!-- SecciÃ³n lista pero oculta hasta que estÃ© la infraestructura web
### ğŸ’° InversiÃ³n

**ConsultorÃ­a:**
- SesiÃ³n de 1 hora: [Precio]
- Paquete 5 horas: [Precio] (ahorras X%)
- Proyecto completo: CotizaciÃ³n personalizada

**CapacitaciÃ³n:**
- Curso bÃ¡sico (40h): [Precio]
- Curso intermedio (60h): [Precio]
- Curso avanzado (80h): [Precio]
- Descuento por grupos: 3+ personas, 15% off

ğŸ“… **Agendar reuniÃ³n:** [Calendly link]
-->

---

## ğŸ¤ Contribuciones

Este repositorio estÃ¡ en constante evoluciÃ³n. Si encuentras:
- ğŸ› Errores o bugs
- ğŸ“ Mejoras en la documentaciÃ³n
- ğŸ’¡ Ideas para nuevos ejercicios
- ğŸ¨ Ejemplos de dashboards

**Crea un Issue o Pull Request:**

1. Fork este repositorio
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

**En resumen:** Puedes usar este material para aprender, enseÃ±ar, o modificar, siempre que des crÃ©dito.

---

## ğŸ™ Agradecimientos

Este repositorio existe gracias a:

- **Mis alumnos** - Cuyas preguntas y feedback mejoran el contenido constantemente
- **NYC Open Data** - Por los datasets pÃºblicos
- **Comunidad de Python** - Pandas, Dask, PySpark developers
- **GitHub** - Por la plataforma que facilita el aprendizaje colaborativo

---

## ğŸ“Š EstadÃ­sticas del Repositorio

![GitHub stars](https://img.shields.io/github/stars/TodoEconometria/ejercicios-bigdata?style=social)
![GitHub forks](https://img.shields.io/github/forks/TodoEconometria/ejercicios-bigdata?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/TodoEconometria/ejercicios-bigdata?style=social)

---

## ğŸš€ Â¿Listo para Empezar?

```bash
# 1. Haz fork de este repositorio (botÃ³n arriba a la derecha)

# 2. Clona TU fork
git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git

# 3. Instala dependencias
cd ejercicios-bigdata
pip install -r requirements.txt

# 4. Empieza con el Ejercicio 01
cd ejercicios
python 01_cargar_sqlite.py

# 5. Â¡Aprende, practica, crece! ğŸ“
```

---

<p align="center">
  <b>Tu carrera en Big Data empieza aquÃ­.</b><br>
  Â¿Preguntas? Abre un <a href="../../issues">Issue</a> o contÃ¡ctame en <a href="https://www.linkedin.com/in/juangutierrezconsultor/">LinkedIn</a>
</p>

<p align="center">
  Hecho con â¤ï¸ por <a href="https://www.todoeconometria.com">TodoEconometria</a>
</p>
